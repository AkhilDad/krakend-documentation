---
lastmod: 2018-11-01
date: 2017-01-21
linktitle: Setting up a cluster
menu:
  documentation:
    parent: cluster
title: Setting up a cluster
weight: 10
notoc: true
---
Hardware can fail at any time, and a Gateway is a piece critical enough to have redundancy of the service. Having a cluster of machines operating the service assures high availability.

Running a cluster of machines is a straightforward process that only requires two conditions:

- Having a balancer in front of the machines (e.g., ELB, Haproxy)
- Run two or more KrakenD services

When all the desired nodes of KrakenD run, all metrics share a universal `cluster ID` between the nodes.

# Autogenerated cluster ID
When KrakenD starts, you can immediately observe in the logs the `cluster ID` information:

    INFO registering usage stats for cluster ID 'CGfjz3BrDgtk+noDcxw18WxxbxtNPb6Rv6C5zArhhG8='

The cluster ID generation is using the calculated `SHA` of the configuration file, meaning that no matter how many nodes of KrakenD you run, all machines belong to the same cluster while they share configuration.

One of the benefits of having a decentralized system is that you can have the same cluster when machines are in different networks or regions, or even when visibility between themselves is not working.

# Adding nodes to the cluster
To add nodes to a cluster of running KrakenD machines, spin up a new service behind the same balancer using the same configuration.

When the machine starts the log shows that the `cluster ID` is the same as you had so far and the machine starts reporting the metrics to the same place, being able to see the new activity.

# Removing nodes from the cluster
Removing a node from the cluster only requires to stop or kill the KrakenD service in the desired node. The balancer makes sure not to send more traffic to the instance.

The machine stops reporting metrics, and this is visible in your observability dashboard.

# Changing to another cluster
Modifying the `krakend.json` file and deploying the changes produces a new `cluster ID`. When all your nodes update to the new configuration, they all share a different common cluster again.